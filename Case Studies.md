Case Studies

Purpose of This Section
The Case Studies section demonstrates how evaluation, risk, operations, and governance frameworks are applied to realistic AI behaviour scenarios.
These case studies are not exhaustive incident logs. They are representative examples designed to show how decisions are made, how risk is interpreted, and why the release was blocked.
This section bridges theory and execution.

What These Case Studies Represent
Each case study illustrates:

  •	A realistic AI behaviour failure mode
  •	How the issue was detected through evaluation
  •	How risk was interpreted using defined frameworks
  •	How operations and governance responded
  •	Why the outcome affected release decisions
The focus is on decision reasoning, not reproduction of failures.

Why Case Studies Matter
Frameworks alone do not demonstrate operational maturity.
Case studies show:

  •	Judgment under ambiguity
  •	Discipline under pressure
  •	How no-ship decisions are justified
  •	How cross-functional inputs are synthesised
  
This section answers the implicit reviewer question:
“Can this person actually run AI Product Operations in practice?”

Structure of Each Case Study
Each case study follows a consistent structure:
  1.	Context and Scenario
  2.	Observed Behaviour
  3.	Evaluation Findings
  4.	Risk Interpretation
  5.	Operational Response
  6.	Decision Outcome
  7.	Key Lessons
This consistency reinforces auditability and governance discipline.

Abstraction and Safety
All case studies are:
  •	Abstracted and anonymised
  •	Free of reproducible prompts
  •	Free of exploit details
  •	Focused on reasoning, not mechanics
This ensures the cases are safe for public sharing.

Case Studies Included

Case 01: Hallucination and False Precision
Demonstrates how precise but uncertain guidance creates trust and safety risk, even without factual error.

Case 02: Escalation and Panic Risk
Demonstrates how over-escalation in ambiguous contexts leads to panic and misuse, triggering no-ship.
Each case maps directly to:
  •	Evaluation Dimensions
  •	Risk Domains
  •	Operations workflows
  •	Governance decisions

Relationship to Other Sections
Case studies draw from and reinforce:
  •	EvalOps: how failures are identified
  •	Behavioural Evaluation: how behaviour is assessed
  •	Risk & Safety: why behaviour matters
  •	Operations: how incidents are handled
  •	Governance: who owns the decision
They do not introduce new rules or exceptions.

What This Section Is Not
This section does not:
  •	Publish raw incident data
  •	Reveal system vulnerabilities
  •	Provide implementation details
  •	Claim completeness of coverage
Its purpose is illustrative clarity, not total disclosure.

Summary
The Case Studies section demonstrates how ZomoBot’s AI Product Operations framework performs under real-world conditions. By walking through concrete examples from detection to decision, these case studies show how responsible AI teams choose not to ship when risk is unresolved.

[Hallucination and False Precision as a Trust and Safety Risk](https://drive.google.com/file/d/12yl58rnGzkm2qAkpAm0JDV7f96ozYf2p/view?usp=drive_link)

[Escalation and Panic Risk in Ambiguous User Contexts](https://drive.google.com/file/d/1EXCDFDvSI3Li3HSlbUk-nhzPZcLvXex7/view?usp=drive_link)

[AI Product Operations – ZomoBot Case Study Summary](https://drive.google.com/file/d/18MhoGjxmOxcUeZtlK-L4z6em-80MtrYT/view?usp=drive_link)
