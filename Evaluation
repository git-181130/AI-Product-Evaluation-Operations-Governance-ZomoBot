EvalOps

Purpose of This Section
The EvalOps section defines how evaluation is performed in this project.
Its role is to establish the rules of judgment before any testing, observation, or decision-making occurs. This ensures that findings are evaluated consistently, severity is interpreted defensibly, and outcomes cannot be reinterpreted after the fact.
EvalOps exists to answer a single core question:
By what standards do we decide whether an AI system’s behaviour is acceptable or not?

What EvalOps Means in This Project
In this project, EvalOps is not a testing checklist or a collection of prompts.
EvalOps is the operational discipline of evaluation, combining:
  •	Clearly defined behavioural expectations
  •	Predefined failure categories
  •	Severity interpretation based on user impact
  •	Structured evaluation summaries that feed into operations
EvalOps ensures that evaluation is systematic, repeatable, and decision-relevant.

AUMI-Aligned Evaluation Focus
This EvalOps framework is AUMI-aligned, meaning it evaluates AI behaviour as an interactive user experience, not a static output.
Accordingly, EvalOps evaluates responses across dimensions such as:
  •	Tone and framing
  •	Response structure and actionability
  •	Escalation proportionality
  •	Misuse and panic-induction risk
  •	Hallucination and false precision
  •	Source trust and citation alignment
A response may be factually correct and still be classified as a failure if it introduces behavioural risk.

Predefined Evaluation Discipline
A core principle of this EvalOps framework is that judgment criteria are fixed before execution.

This includes:
  •	Evaluation charters defining acceptable and unacceptable behaviour
  •	Severity interpretation rules
  •	Category planning for different risk contexts
  •	Minimum expected outputs for safe responses
Once evaluation begins, these definitions do not change.
This prevents outcome-driven standards and ensures credibility.

Severity Interpretation
Severity in this project reflects potential user impact, not model intent or effort.
Severity increases when behaviour could:
  •	Influence unsafe real-world action
  •	Induce unnecessary fear, panic, or over-reliance
  •	Remove user agency through authoritative framing
  •	Present uncertain guidance as guaranteed
A single unresolved high-impact behavioural failure is sufficient to block release.

EvalOps Artifacts in This Section
This folder contains the core evaluation artifacts used throughout the project, including:
  •	Evaluation Charter: defines evaluation authority, scope, and principles
  •	Evaluation Dimensions: behavioural dimensions used across assessments
  •	Category Plan: risk-based grouping of evaluation scenarios
  •	Minimum Expected Outputs: baseline safety expectations
  •	Severity Interpretation: how findings are classified and weighted
  •	Failure Pattern Mapping: systemic behavioural risk patterns
  •	Evaluation Summary: consolidated findings and decision inputs
Each artifact is designed to be reviewable independently and traceable to downstream decisions.

What EvalOps Does Not Do
EvalOps in this project does not:
  •	Optimize responses or tune prompts
  •	Measure model intelligence or accuracy
  •	Guarantee safety or future behaviour
  •	Override operational or governance decisions
EvalOps provides evidence, not approval.

Why EvalOps Matters
Without EvalOps discipline:
  •	Evaluation becomes subjective
  •	Severity becomes negotiable
  •	No-ship decisions appear arbitrary
  •	Risk is discovered post-release
EvalOps ensures that not shipping is a legitimate, explainable, and defensible outcome.

Summary
EvalOps is the backbone of this project.
By defining how behaviour is evaluated before observing outcomes, this framework ensures that decisions are consistent, evidence-led, and resistant to post-hoc justification.
In safety- and trust-sensitive AI products, this discipline is not optional, it is foundational.

